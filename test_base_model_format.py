#!/usr/bin/env python3
"""
æµ‹è¯•Base Modelæ ¼å¼ä¿®æ”¹
éªŒè¯å»æ‰chat templateåçš„promptæ ¼å¼
"""

import sys
import os
sys.path.append('/home/xz2649/projects/FollowBench/code')

from model_inference_vllm import generate_prompts
from transformers import AutoTokenizer

def test_prompt_generation():
    """æµ‹è¯•promptç”Ÿæˆ"""
    print("ğŸ§ª æµ‹è¯•Base Model Promptç”Ÿæˆ")
    print("=" * 50)
    
    # æ¨¡æ‹ŸFollowBenchæ•°æ®
    test_data = [
        {
            'prompt_new': 'Write a story about a cat. The story must be exactly 3 sentences long.'
        },
        {
            'prompt_new': 'List 5 animals. Each animal name must start with the letter "B".'
        },
        {
            'prompt_new': 'Explain photosynthesis in simple terms. Use exactly 2 paragraphs.'
        }
    ]
    
    print("ğŸ“ æµ‹è¯•æ•°æ®:")
    for i, example in enumerate(test_data, 1):
        print(f"  {i}. {example['prompt_new'][:50]}...")
    
    print("\nğŸ”„ ç”Ÿæˆprompts...")
    
    # æµ‹è¯•ä¸åŒtokenizerçš„æƒ…å†µ
    tokenizers_to_test = [
        ('æ— tokenizer', None),
        ('DeepSeek Base', 'deepseek-ai/deepseek-llm-7b-base'),
        ('DeepSeek Chat', 'deepseek-ai/deepseek-llm-7b-chat'),
    ]
    
    for name, model_path in tokenizers_to_test:
        print(f"\nğŸ” æµ‹è¯• {name}:")
        print("-" * 30)
        
        try:
            if model_path:
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                has_template = hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None
                print(f"  Chat template: {'âœ… æœ‰' if has_template else 'âŒ æ— '}")
            else:
                tokenizer = None
                print("  Chat template: âŒ æ— tokenizer")
            
            # ç”Ÿæˆprompts
            prompts = generate_prompts(test_data, tokenizer)
            
            print(f"  ç”Ÿæˆçš„prompts:")
            for i, prompt in enumerate(prompts, 1):
                print(f"    {i}. {repr(prompt[:60])}...")
                
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")

def compare_old_vs_new_format():
    """æ¯”è¾ƒæ—§æ ¼å¼vsæ–°æ ¼å¼"""
    print("\nâš–ï¸ æ ¼å¼å¯¹æ¯”")
    print("=" * 50)
    
    test_prompt = "Write a story about a cat. The story must be exactly 3 sentences long."
    
    print("ğŸ”´ æ—§æ ¼å¼ (ä½¿ç”¨chat template):")
    try:
        tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/deepseek-llm-7b-chat', trust_remote_code=True)
        old_format = tokenizer.apply_chat_template(
            [{"role": "user", "content": test_prompt}], 
            tokenize=False, 
            add_generation_prompt=True
        )
        print(f"  {repr(old_format[:100])}...")
    except Exception as e:
        print(f"  é”™è¯¯: {e}")
    
    print("\nğŸŸ¢ æ–°æ ¼å¼ (ç®€å•æ ¼å¼):")
    new_format = test_prompt
    print(f"  {repr(new_format)}")
    
    print("\nğŸ“Š å¯¹æ¯”åˆ†æ:")
    print("  æ—§æ ¼å¼ç‰¹ç‚¹:")
    print("    - åŒ…å«ç‰¹æ®Šæ ‡è®° (<|im_start|>, <|im_end|>ç­‰)")
    print("    - å¯¹è¯ç»“æ„åŒ–æ ¼å¼")
    print("    - é€‚åˆinstructæ¨¡å‹")
    print("  æ–°æ ¼å¼ç‰¹ç‚¹:")
    print("    - çº¯æ–‡æœ¬ï¼Œæ— ç‰¹æ®Šæ ‡è®°")
    print("    - ç›´æ¥çš„æŒ‡ä»¤æ ¼å¼")
    print("    - é€‚åˆbaseæ¨¡å‹")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ FollowBench Base Modelæ ¼å¼æµ‹è¯•")
    print("=" * 60)
    
    test_prompt_generation()
    compare_old_vs_new_format()
    
    print("\nâœ… ä¿®æ”¹æ€»ç»“:")
    print("=" * 50)
    print("1. âœ… å»æ‰äº†chat templateçš„ä½¿ç”¨")
    print("2. âœ… ä½¿ç”¨ç®€å•çš„åŸå§‹promptæ ¼å¼")
    print("3. âœ… é€‚åˆbase modelçš„è®­ç»ƒæ•°æ®æ ¼å¼")
    print("4. âœ… é¿å…äº†base modelçœ‹åˆ°æœªè§è¿‡çš„å¯¹è¯æ ‡è®°")
    
    print("\nğŸš€ ç°åœ¨å¯ä»¥è¿è¡ŒFollowBenchæµ‹è¯•:")
    print("  python3 model_inference_vllm.py --model_path <base_model_path> ...")

if __name__ == "__main__":
    main()
