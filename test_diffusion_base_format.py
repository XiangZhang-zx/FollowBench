#!/usr/bin/env python3
"""
æµ‹è¯•Dreamå’ŒLLaDAçš„Base Modelæ ¼å¼ä¿®æ”¹
éªŒè¯å»æ‰chat templateåçš„æ¨ç†æ˜¯å¦æ­£å¸¸
"""

import json
import tempfile
import os
from transformers import AutoTokenizer

def test_dream_format():
    """æµ‹è¯•Dreamçš„æ ¼å¼å¤„ç†"""
    print("ğŸ§ª æµ‹è¯•Dream Base Modelæ ¼å¼")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    test_instruction = "Write a story about a cat. The story must be exactly 3 sentences long."
    
    print(f"ğŸ“ æµ‹è¯•æŒ‡ä»¤: {test_instruction}")
    
    try:
        # åŠ è½½Dream tokenizer
        tokenizer = AutoTokenizer.from_pretrained('Dream-org/Dream-v0-Base-7B', trust_remote_code=True)
        
        print("\nğŸ”´ æ—§æ ¼å¼ (ä½¿ç”¨chat template):")
        try:
            messages = [{'role': 'user', 'content': test_instruction}]
            old_inputs = tokenizer.apply_chat_template(
                messages, return_tensors='pt', return_dict=True, add_generation_prompt=True
            )
            print(f"  Input IDs shape: {old_inputs['input_ids'].shape}")
            print(f"  Decoded: {repr(tokenizer.decode(old_inputs['input_ids'][0][:50]))}...")
        except Exception as e:
            print(f"  é”™è¯¯: {e}")
        
        print("\nğŸŸ¢ æ–°æ ¼å¼ (ç®€å•æ ¼å¼):")
        new_inputs = tokenizer(test_instruction, return_tensors='pt', return_dict=True)
        print(f"  Input IDs shape: {new_inputs['input_ids'].shape}")
        print(f"  Decoded: {repr(tokenizer.decode(new_inputs['input_ids'][0][:50]))}...")
        
        print("\nğŸ“Š å¯¹æ¯”:")
        print("  æ—§æ ¼å¼: åŒ…å«å¯¹è¯æ ‡è®°ï¼Œæ›´é•¿")
        print("  æ–°æ ¼å¼: çº¯æŒ‡ä»¤æ–‡æœ¬ï¼Œæ›´ç®€æ´")
        
    except Exception as e:
        print(f"âŒ Dream tokenizeråŠ è½½å¤±è´¥: {e}")

def test_llada_format():
    """æµ‹è¯•LLaDAçš„æ ¼å¼å¤„ç†"""
    print("\nğŸ§ª æµ‹è¯•LLaDA Base Modelæ ¼å¼")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    test_instruction = "List 5 animals. Each animal name must start with the letter B."
    
    print(f"ğŸ“ æµ‹è¯•æŒ‡ä»¤: {test_instruction}")
    
    try:
        # åŠ è½½LLaDA tokenizer
        tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Base', trust_remote_code=True)
        
        print("\nğŸ”´ æ—§æ ¼å¼ (å¦‚æœä½¿ç”¨chat template):")
        try:
            messages = [{'role': 'user', 'content': test_instruction}]
            old_inputs = tokenizer.apply_chat_template(
                messages, return_tensors='pt', return_dict=True, add_generation_prompt=True
            )
            print(f"  Input IDs shape: {old_inputs['input_ids'].shape}")
            print(f"  Decoded: {repr(tokenizer.decode(old_inputs['input_ids'][0][:50]))}...")
        except Exception as e:
            print(f"  é”™è¯¯: {e}")
        
        print("\nğŸŸ¢ æ–°æ ¼å¼ (LLaDAå·²ç»ä½¿ç”¨ç®€å•æ ¼å¼):")
        # LLaDAæ¨ç†è„šæœ¬ä¸­ç›´æ¥ä½¿ç”¨: inputs = [item['prompt_new'] for item in data_shard]
        print(f"  ç›´æ¥ä½¿ç”¨åŸå§‹æŒ‡ä»¤: {repr(test_instruction)}")
        print("  âœ… LLaDAæ¨ç†è„šæœ¬å·²ç»æ­£ç¡®ä½¿ç”¨ç®€å•æ ¼å¼")
        
    except Exception as e:
        print(f"âŒ LLaDA tokenizeråŠ è½½å¤±è´¥: {e}")

def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶"""
    print("\nğŸ“ åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶")
    print("=" * 50)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ•°æ®
    test_data = [
        {
            "prompt_new": "Write a story about a cat. The story must be exactly 3 sentences long.",
            "constraint_type": "content"
        },
        {
            "prompt_new": "List 5 animals. Each animal name must start with the letter B.",
            "constraint_type": "format"
        }
    ]
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæ–‡ä»¶
    test_dir = "/tmp/followbench_test"
    os.makedirs(test_dir, exist_ok=True)
    
    test_file = os.path.join(test_dir, "content_constraint.jsonl")
    with open(test_file, 'w', encoding='utf-8') as f:
        for item in test_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… æµ‹è¯•æ•°æ®å·²åˆ›å»º: {test_file}")
    print(f"ğŸ“ åŒ…å« {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    return test_file, test_dir

def simulate_inference_test():
    """æ¨¡æ‹Ÿæ¨ç†æµ‹è¯•"""
    print("\nğŸš€ æ¨¡æ‹Ÿæ¨ç†æµ‹è¯•")
    print("=" * 50)
    
    test_file, test_dir = create_test_data()
    
    print("ğŸ“‹ æµ‹è¯•å‘½ä»¤ç¤ºä¾‹:")
    print("\n1. Dreamæ¨ç†æµ‹è¯•:")
    dream_cmd = f"""python3 dream_inference.py \\
    --model_path Dream-org/Dream-v0-Base-7B \\
    --constraint_types content \\
    --api_input_path {os.path.dirname(test_file)} \\
    --api_output_path {test_dir}/dream_output \\
    --max_new_tokens 256 \\
    --diffusion_steps 64"""
    print(dream_cmd)
    
    print("\n2. LLaDAæ¨ç†æµ‹è¯•:")
    llada_cmd = f"""python3 llada_inference.py \\
    --model_path GSAI-ML/LLaDA-8B-Base \\
    --constraint_types content \\
    --api_input_path {os.path.dirname(test_file)} \\
    --api_output_path {test_dir}/llada_output \\
    --max_new_tokens 256 \\
    --diffusion_steps 64"""
    print(llada_cmd)
    
    print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶ä½ç½®: {test_file}")
    print("ğŸ’¡ æç¤º: è¿è¡Œä¸Šè¿°å‘½ä»¤æ¥æµ‹è¯•å®é™…æ¨ç†")

def verify_modifications():
    """éªŒè¯ä¿®æ”¹æ˜¯å¦æ­£ç¡®"""
    print("\nâœ… ä¿®æ”¹éªŒè¯")
    print("=" * 50)
    
    modifications = [
        {
            "æ–‡ä»¶": "dream_inference.py",
            "ä¿®æ”¹": "å»æ‰apply_chat_templateï¼Œä½¿ç”¨tokenizer(instruction, ...)",
            "çŠ¶æ€": "âœ… å·²ä¿®æ”¹"
        },
        {
            "æ–‡ä»¶": "llada_inference.py", 
            "ä¿®æ”¹": "å·²ç»ä½¿ç”¨ç®€å•æ ¼å¼: inputs = [item['prompt_new'] for item in data_shard]",
            "çŠ¶æ€": "âœ… æ— éœ€ä¿®æ”¹"
        },
        {
            "æ–‡ä»¶": "model_inference_vllm.py",
            "ä¿®æ”¹": "å¼ºåˆ¶ä½¿ç”¨ç®€å•æ ¼å¼ï¼Œä¸ä½¿ç”¨chat template",
            "çŠ¶æ€": "âœ… å·²ä¿®æ”¹"
        }
    ]
    
    for mod in modifications:
        print(f"ğŸ“„ {mod['æ–‡ä»¶']}:")
        print(f"  ä¿®æ”¹å†…å®¹: {mod['ä¿®æ”¹']}")
        print(f"  çŠ¶æ€: {mod['çŠ¶æ€']}")
        print()
    
    print("ğŸ¯ ä¿®æ”¹æ€»ç»“:")
    print("1. âœ… æ‰€æœ‰æ¨ç†è„šæœ¬éƒ½å·²å»æ‰chat template")
    print("2. âœ… ä½¿ç”¨é€‚åˆbase modelçš„ç®€å•æ ¼å¼")
    print("3. âœ… é¿å…äº†å¯¹è¯æ ‡è®°å¯¼è‡´çš„è¾“å‡ºé—®é¢˜")
    print("4. âœ… ç¡®ä¿base modelèƒ½æ­£å¸¸ç”Ÿæˆå“åº”")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ FollowBench Diffusionæ¨¡å‹Baseæ ¼å¼æµ‹è¯•")
    print("=" * 60)
    
    test_dream_format()
    test_llada_format()
    simulate_inference_test()
    verify_modifications()
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("ç°åœ¨å¯ä»¥ä½¿ç”¨base modelè¿è¡ŒFollowBenchæ¨ç†ï¼Œä¸ä¼šæœ‰chat templateé—®é¢˜")

if __name__ == "__main__":
    main()
